# Halloween Sentinel

A macOS-focused Python app that eavesdrops for speech, snaps photos every few seconds, and conjures a spooky narration whenever a person appears on camera. The narration is generated by GPT with your own haunted-house prompt and spoken aloud with ElevenLabs.

## Features
- Continuous audio capture with VAD trimming; transcripts forwarded to OpenAI Whisper for transcription.
- Face detection from the default webcam every 10 seconds (configurable).
- When a face is found, the latest transcript is sent to ChatGPT with a haunted system prompt and a throttled stream of photos.
- Replies are spoken using ElevenLabs' text-to-speech for an eerie voice-over.
- Maintains a rolling conversation so the spirit remembers recent exchanges and resets after 20 seconds of silence.

## Prerequisites
- Python 3.10+ (recommended 3.11).
- PortAudio installed (`brew install portaudio`) for `sounddevice`.
- OpenCV runtime dependencies (`brew install opencv`).
- OpenAI Whisper access (e.g., model `whisper-1` or `gpt-4o-mini-transcribe`) using the same `OPENAI_API_KEY` as GPT or `WHISPER_API_KEY`.
- OpenAI API key with access to a multimodal GPT model.
- ElevenLabs API key and voice ID.

## Setup
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Create a `.env` file in the project root (or export variables another way):
```bash
OPENAI_API_KEY=sk-...
OPENAI_GPT_MODEL=gpt-4.1-mini
WHISPER_MODEL=whisper-1
ELEVENLABS_API_KEY=eleven-...
ELEVENLABS_VOICE_ID=your_spooky_voice_id
```

Optional overrides:
- `CAPTURE_INTERVAL_SECONDS` (default `10`)
- `VOICE_SILENCE_TIMEOUT` (default `1.0` seconds)
- `VOICE_MIN_MS` (default `600` ms)
- `SPOOKY_PROMPT` to rewrite the haunted narration template
- `WHISPER_MODEL` to pick a different transcription model
- `WHISPER_API_KEY` if you prefer a dedicated key
- `CAPTURE_OUTPUT_DIR` to change where photos land (default `./captures`)
- `VOICE_INPUT_DEVICE` to force a specific microphone (name or index as reported by PortAudio)

## Running
```bash
python -m src.main
```
The app will:
1. Start listening for speech and collecting transcripts.
2. Grab a webcam frame every `CAPTURE_INTERVAL_SECONDS`.
3. When it detects a face, send the photo plus recent transcript to GPT.
4. Speak GPT's reply through ElevenLabs.

Stop with `Ctrl+C`.

## Notes
- The `captures/` folder fills with timestamped JPEGs you can inspect later.
- Whisper, GPT, and ElevenLabs clients log errors but keep the app alive so transient failures do not crash the show.
- Detection uses OpenCV's Haar cascade; swap in a stronger detector if you need higher accuracy.
- Transcription uses OpenAI's hosted Whisper API, so no local server is required.

### Audio routing tips

If you connect a Bluetooth speaker that also exposes a microphone, macOS may switch both the input and output devices. The app now lets you pin the input device so you can keep the built-in Mac microphone while sending playback to the Bluetooth speaker:

1. List available audio devices (names and indices) once your gear is connected:
   ```bash
   python - <<'PY'
   import sounddevice as sd
   for idx, dev in enumerate(sd.query_devices()):
       if dev.get("max_input_channels", 0) > 0:
           print(f"{idx}: {dev['name']} (inputs: {dev['max_input_channels']})")
   PY
   ```
2. Set `VOICE_INPUT_DEVICE` in your `.env` to either the index or a substring of the microphone name, e.g.:
   ```bash
   VOICE_INPUT_DEVICE="MacBook Pro Microphone"
   ```
3. macOS System Settings → Sound → Output: pick your Bluetooth speaker. Input can stay on “MacBook Pro Microphone” (or whatever mic you pinned).

Playback uses `afplay`, which follows the system output device, so narration automatically routes to whatever you set for output.


## VS Code Setup

1. Install the recommended extensions when VS Code prompts (Python + Pylance).
2. Open the Command Palette (⇧⌘P) and run "Python: Select Interpreter".
3. Choose the interpreter pointing to `.venv/bin/python` (auto-detected via settings).
4. Open a terminal in VS Code; it activates the virtualenv automatically.
5. Run `pip install -r requirements.txt` inside the activated environment.
